{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Features e Construção do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introdução a Extração de Features\n",
    "- A extração de features é uma parte muito importante na análise e na identificação de relações entre diferentes elementos. Como já sabemos, os dados de áudio não podem ser compreendidos diretamente pelos modelos, então precisamos convertê-los para um formato inteligível, e é para isso que a extração de features é utilizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com a taxa de amostragem e os dados do sinal, podemos realizar diversas transformações para extrair características valiosas do áudio. No entanto, neste projeto, não vamos aprofundar no processo de seleção de features para identificar quais são mais relevantes para o nosso dataset. Em vez disso, vamos extrair cinco features principais para treinar nosso modelo:\n",
    "- **Zero Crossing Rate (ZCR):** Mede a taxa de mudanças de sinal no áudio, ou seja, quantas vezes ele cruza o eixo zero em um determinado intervalo de tempo. Essa feature é útil para distinguir sons percussivos e não percussivos.\n",
    "- **Chroma STFT:** Representa a energia espectral em 12 bins correspondentes às notas da escala musical ocidental. Essa característica é útil para identificar padrões harmônicos no áudio.\n",
    "- **MFCC (Mel-Frequency Cepstral Coefficients):** Converte a frequência do áudio para a escala mel, aproximando-se da percepção auditiva humana. É uma das features mais utilizadas em reconhecimento de fala e emoção.\n",
    "- **RMS (Root Mean Square):** Mede a energia do sinal ao calcular a média quadrática das amplitudes do áudio. Essa feature ajuda a representar a intensidade do som.\n",
    "- **Mel Spectrogram:** Representa a distribuição de energia do áudio em diferentes faixas de frequência na escala mel, capturando informações espectrais essenciais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Carregar o Dataframe da EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/Vinícius/Desktop/miniprojeto Trilha/miniprojeto2/data/ravdess_preprocessed.csv\" \n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Criando uma função para extração das features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fizemos essa parte no notebook passado, então você pode copiar e colar o código das funções aqui, pois precisaremos delas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    n = np.random.normal(0, data.std(), data.size) # cria um vetor com ruído branco \n",
    "    return data + n # retorna o áudio com ruido\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate=rate) # retonra um áudio com velocidade alterada\n",
    "\n",
    "def shift(data):\n",
    "    return np.roll(data, np.random.random(1, len(data))) # retorna um vetor de audio com os dados deslocados aleatoriamente\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor) # retorna um audio com a frequência alterada\n",
    "\n",
    "# Tomando um áudio qualquer com exemplo\n",
    "path = np.array(df.Path)[1]\n",
    "data, sample_rate = librosa.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aqui você deve extrair essas features de fato... mais tarde você precisará delas :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funções usadas**\n",
    "\n",
    "### **vstack()**: concatena dois arrays numpy por linha e gera uma matriz\n",
    "\n",
    "[1, 2, 3] e [4, 5, 6] -> [[1, 2, 3,]\n",
    "                          [4, 5, 6,]]  \n",
    "\n",
    "### **hstack()**: concatena pela coluna e gera uma matriz.\n",
    "\n",
    "[1, 2, 3] e [4, 5, 6] -> [[1, 2, 3, 4, 5, 6]]  \n",
    "\n",
    "\n",
    "### **zero.crossings()**: calcula quantos ZCR existem em um áudio.  \n",
    "- Retorna um array numpy de **tamanho igual ao vetor de áudio**, em que cada elemento indica se **passou pelo eixo X ou não** \n",
    "- **Parâmetros**: (audio, pad=False), pan=False deixa de contar com o primeiro valor do array audio[0]  \n",
    "  \n",
    "### **chroma_stft()** retorna o espectograma de notas  \n",
    "- **Parâmetros**: (audio, sample rate)\n",
    "- Com o espectograma, é possível descobrir a tonalidade de uma música\n",
    "- Para descobrir quantos existem em um áudio: utilizar a função sum  \n",
    "\n",
    " ### **np.mean()**: Para cada feature, é necessário reduzir sua dimensão, pois a função hstack só aceita arrays com o mesmo numero de linhas (). Para isso, tomamos a média dos valores de cada feature para jogar no ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Escala mel**: escala que tenta simular numericamente a percepção humana do som\n",
    "- Mudanças de sons em baixa frequência são mais fáceis de serem percebidos, mas mudança em sons de alta frequência quase não percebemos\n",
    "\n",
    "### **.mfcc()**: retorna um áudio em escala mel para cada frame do áudio \n",
    "- **Parâmetros**: (audio, sr, n_mfcc)\n",
    " - **n_mfcc** é a quantidade de dados que iremos tomar sobre o audio na escala mel. Tomamos 13 porque já são suficientes para detectar padrões e é um bom número para treinar modelos de machine learning com desempenho  \n",
    "\n",
    "### **.melspectogram()**: retorna um array com o espectograma do áudio em escala mel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, sample_rate): \n",
    "    result = np.array([])\n",
    "    # zcr\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data, pad=False), axis=1)\n",
    "    result = np.hstack((result, zcr))\n",
    "\n",
    "    # Chroma_stft\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=data, sr=sample_rate), axis=1)\n",
    "    result = np.hstack((result, chroma_stft)) \n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate), axis=1) # n_mfcc = 20, pois com 20 amostras já é possível detectar padrões no áudio \n",
    "    result = np.hstack((result, mfcc))\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data, frame_length=2048, hop_length=512), axis=1)\n",
    "    result = np.hstack((result, rms))\n",
    "\n",
    "    # MelSpectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr = sample_rate), axis=1)\n",
    "    result = np.hstack((result, mel))\n",
    "    \n",
    "    return result # retorna um array com 1x162, em que cada coluna é o dado de uma feaure \n",
    "\n",
    "def get_features(path):\n",
    "    # Carregar áudio\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "\n",
    "    # Sem aumento de dados\n",
    "    res1 = extract_features(data, sample_rate)  \n",
    "    result = np.array(res1)\n",
    "\n",
    "    # Com ruído\n",
    "    noise_data = noise(data) # aplica áudio com ruído\n",
    "    res2 = extract_features(noise_data, sample_rate)  \n",
    "    result = np.vstack((result, res2))\n",
    "\n",
    "    # Com alongamento e mudança de pitch\n",
    "    new_data = stretch(data) # altera o tempo\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate) # altera a afinação (pitch) \n",
    "    res3 = extract_features(data_stretch_pitch, sample_rate) \n",
    "    result = np.vstack((result, res3)) # result é o empilhamento de linhas de todos as features\n",
    "    \n",
    "    return result # retorna um array 3x162, em que todas as linhas representam o mesmo áudio e as linhas são as features desse áudio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rode as células abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideia do código:  \n",
    "   \n",
    "1° Itera em cada linha do dataset capturando o path do áudio e a emoção associada.  \n",
    "\n",
    "2° Chama a função que captura as features de um áudio\n",
    "- A função retorna uma matriz 3x5, em que **a primeira linha é o áudio normal, a segunda é o áudio com ruído e a terceira é o áudio com alteração no pitch+stretch**\n",
    "- As colunas são as 5 features associadas a cada áudio  \n",
    "\n",
    "3° Adiciona cada linha, ou seja, cada **informação sobre a feature** na lista X\n",
    "\n",
    "4° Adiciona a emoção associada a essa feature na lista Y\n",
    "\n",
    "\n",
    "Assim, cada elemento de X se relaciona com sua respectiva emoção salva em Y. Exemplo:  \n",
    "\n",
    "X = [ [1, 2, 3, 4, 5], [4, 3, 1, 6, 7], [7, 8, 6, 4, 1], [4, 3, 2, 1, 5], [12, 34, 66, 88, 11], [32, 14, 53, 67, 12] ]  \n",
    "\n",
    "Y = [ 'fear', 'fear', 'fear', 'happy', 'happy', 'happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X salva as features do áudio\n",
    "# Y salva as emoções associadas as features\n",
    "X, Y = [], []\n",
    "\n",
    "# zip retorna uma dupla em que cada elemento é uma dupla: um de path e outro de emotions\n",
    "for path, emotion in zip(df['Path'], df['Emotions']):\n",
    "    feature = get_features(path) # retorna um array 3x5 [ [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5] ]\n",
    "\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        Y.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # essa linha retorna uma tupla que informa o tamanho de X, Y, número de áudios\n",
    "# como cada áudio sofre data augmentation 3x, então o tamanho de X  Y é 3 vezes maior que o número de áudios \n",
    "\n",
    "len(X), len(Y), df.Path.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agora você deverá salvar o csv de features (features.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicação do código**:\n",
    "\n",
    "Após extrair X e Y, temos as informações de cada feature associada a uma única emoção. Agora, a ideia é criar um DataSet em que cada linha contenha as features de um áudio e sua respectiva emoção "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.dirname() retorna o nome de um diretório\n",
    "# os.getcwd() retorna o diretório pai \n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\") # acessa o diretório pai na pasta 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# armazena a informação de onde será salvo o CSV criado\n",
    "csv_path = os.path.join(data_dir, \"features.csv\")\n",
    "\n",
    "# Crie um DataFrame do pandas chamado Features usando a lista X.\n",
    "Features = pd.DataFrame(X)\n",
    "\n",
    "# Transformando Y em um dataset com uma única coluna\n",
    "labels = pd.DataFrame(Y, columns=['labels'])\n",
    "\n",
    "# Concatenando o dataset labels a features\n",
    "Features = pd.concat([Features, labels], axis=1)\n",
    "\n",
    "# Salve o DataFrame como um arquivo CSV no caminho definido, sem incluir o índice, definindo _index_=False\n",
    "Features.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"O arquivo foi salvo em: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Preparation (Preparação dos Dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funções usadas:**  \n",
    "\n",
    "### **.iloc()**: serve para selecionar linhas e colunas específicas de um dataframe. Exemplo:  \n",
    "- .iloc[0:10, -1]: seleciona as linhas de 0 a 4 da última coluna. Usa-se a vírgula para separar linha/coluna \n",
    "- .iloc[0, :-1]: seleciona a primeira linha de todas as colunas menos a última  \n",
    "- **Retorno**: **uma series pandas**\n",
    "\n",
    "\n",
    "### **.get_dummies()**: serve para transformar um array numpy em dados categoricos\n",
    "- **Parâmetros**: (array numpy, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleciona todas as linhas da primeira até a penultima coluna\n",
    "# aplica .values para que cada elemento de X seja uma linha do dataset \n",
    "X = Features.iloc[: ,:-1].values \n",
    "\n",
    "# Retorna todos os dados de labels em um array\n",
    "Y = Features['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder para transformar o Y em uma representação binária categórica, necessária para problemas de classificação multiclasse\n",
    "Y = pd.get_dummies(Y, dtype= int) \n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **train_test_split**: divide cada matriz passada em duas: dados para teste e dados para treino  \n",
    "\n",
    "- **Parâmetros**: (matriz1, matriz2, ..., train_size=, test_size=)\n",
    "- Divide **ALEATORIAMENTE** quais dados usará no conjunto teste ou treino\n",
    "- Mesmo passando duas matrizes, ela vai dividir as **mesmas linhas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split do scikit-learn para dividir X e Y em conjuntos de treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15) # divide em 15% dos dados para teste e 85% para treino\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StandardScaler**: é uma fórmula para normalizar os dados:\n",
    "\n",
    "z = (X - média da coluna de X) / desvio padrão de X  \n",
    "\n",
    "**.fit()**: calcula o novo valor para os dados  \n",
    "  \n",
    "**.transform()**: aplica a transformação de valores  \n",
    "\n",
    "Em X_test não é necessário usar o .fit(), pois o calculo ja foi feito anteriormente, so basta aplicá-lo com transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize StandardScaler do sklearn para normalizar as características de X\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rode essa célula para deixar as dimensões certinhas com o modelo que iremos criar.\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training (Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O modelo que vamos usar é uma rede neural convolucional (CNN) projetada para processar os dados extraídos dos arquivos de áudio. Essa estrutura é ideal para capturar padrões espectrais, como variações de tom e intensidade. A CNN é composta por:\n",
    "- Camadas Convolucionais (Conv1D): Extraem características do espectro do áudio.\n",
    "- Camadas de Pooling (MaxPooling1D): Reduzem a dimensionalidade e capturam as informações mais relevantes.\n",
    "- Dropout: Ajuda a evitar overfitting.\n",
    "- Camada Flatten: Transforma os mapas de features em um vetor de entrada para a camada totalmente conectada.\n",
    "- Camadas Densas (Dense): Realizam a classificação final usando a função de ativação softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma imagem é uma grande matriz. Em que cada ponto é uma cor RGB. Assim, computar todos os pontos dessa matriz é algo extremamente custoso.  \n",
    "\n",
    "A rede CNN **tenta extrair as características mais importantes em uma imagem**. Exemplo, ao se deparar com a imagem de ondas sonoras de um áudio, tenta computar somente as partes que as ondas aparecem  \n",
    "\n",
    "# **Funções usadas:**  \n",
    "\n",
    "### **keras.sequential()**: a rede CNN utilizada possui uma única entrada. A função serve para escrever camadas em Redes desse tipo  \n",
    "\n",
    "### **keras.layers.Conv1D()**: cria uma camada de CNN para 1 dimensão  \n",
    "\n",
    "Em uma rede CNN, é importante saber quais áreas da imagem são as mais importantes:  \n",
    "### **keras.layers.MaxPooling1D** é a função que reduz a imagem na CNN  \n",
    "\n",
    "## **keras.layers.Dropout**: elimina aleatoriamente algumas conexões da rede de CNN\n",
    "- O objetivo é impedir o **Overffiting**, pois, ao eliminar conexões, diminui a probabilidade de o modelo estar excessivamente treinado com seus próprios dados \n",
    "- mais infos: https://medium.com/@csediveyanand/what-is-dropout-in-keras-deep-learning-c2d0913439f3\n",
    "\n",
    "### **keras.Flatten**: ao entrar com imagens de diferentes dimensões, o Flatten as converte para um array unidimensional  \n",
    "- Isso acontece para que o modelo só precise de um único neurônio de entrada\n",
    "- mais infos: https://kevinmlean.medium.com/how-to-use-keras-layers-flatten-c3f29ed1b686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">162</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m162\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m1,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_12 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m327,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_13 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_16 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_14 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_17 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m41,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_15 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m704\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m22,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">557,288</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m557,288\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">557,288</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m557,288\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dica: você pode olhar a documentação e ir seguindo o passo a passo arquitetônico para criar o modelo.\n",
    "\n",
    "# Passo 1: Use Sequential() para criar o modelo como um contêiner linear.\n",
    "# Passo 2: Adicione uma camada Conv1D com 256 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 3: Siga com uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 4: Adicione mais uma camada Conv1D com 256 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 5: Adicione mais uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 6: Adicione mais uma camada Conv1D com 128 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 7: Adicione mais uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 8: Adicione uma camada Dropout com 0.2 de taxa de dropout.\n",
    "# Passo 9: Adicione mais uma camada Conv1D com 64 filtros, kernel_size=5, strides=1, padding='same' e função de ativação 'relu'.\n",
    "# Passo 10: Adicione mais uma camada MaxPooling1D com pool_size=5, strides=2, padding='same'.\n",
    "# Passo 11: Adicione uma camada Flatten.\n",
    "# Passo 12: Adicione uma camada Dense com 32 unidades e função de ativação 'relu'.\n",
    "# Passo 13: Adicione uma camada Dropout com 0.3 de taxa de dropout.\n",
    "# Passo 14: Adicione uma camada Dense com 8 unidades e função de ativação 'softmax'.\n",
    "# Passo 15: Compile o modelo com otimizador 'adam', loss 'categorical_crossentropy' e métrica 'accuracy'.\n",
    "# Passo 16: Use model.summary() para visualizar o modelo.\n",
    "\n",
    "modelo = keras.Sequential( # passo 1\n",
    "    [\n",
    "     keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding='same', activation='relu',input_shape=(162, 1)), #neurônio de entrada # passo 2\n",
    "     keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same'), # passo 3\n",
    "     keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding='same', activation='relu'), # passo 4\n",
    "     keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same'), # passo 5\n",
    "     keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding='same', activation='relu'), # passo 6\n",
    "     keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same'), # passo 7\n",
    "     keras.layers.Dropout(rate=0.2), # passo 8\n",
    "     keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'), # passo 9\n",
    "     keras.layers.MaxPooling1D(pool_size=5, strides=2, padding='same'), # passo 10\n",
    "     keras.layers.Flatten(), # passo 11\n",
    "     keras.layers.Dense(units=32,activation='relu'), # passo 12\n",
    "     keras.layers.Dropout(rate=0.3), # passo 13\n",
    "     keras.layers.Dense(units=8,activation='softmax')\n",
    "                                                    ]\n",
    ")\n",
    "\n",
    "# adam é um otimizador para atualizar os pesos durante o treino\n",
    "# loss é a função de perda, ou seja, o que o modelo deve minizar (diferença entre o previsto e o real)\n",
    "modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #passo 15\n",
    "modelo.summary() #passo 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Agora vamos de fato treinar o modelo, siga esses passos: \n",
    "##### 1. Use o callback para Ajustar a Taxa de Aprendizado \n",
    "##### ReduceLROnPlateau: Diminui a taxa de aprendizado quando uma métrica está estagnada.\n",
    "   Parâmetros:\n",
    "   - monitor='loss': Monitora a perda durante o treinamento.\n",
    "   - factor=0.4: Reduz a taxa de aprendizado por este fator.\n",
    "   - patience=2: Número de épocas sem melhora antes da redução.\n",
    "   - min_lr=0.0000001: Limite mínimo para a taxa de aprendizado.\n",
    "             \n",
    "##### 2. Treine o Modelo \n",
    "##### Utilize model.fit para iniciar o treino da rede neural.\n",
    "   Parâmetros:\n",
    "   - x_train, y_train: Conjunto de dados de treino.\n",
    "   - batch_size=64: Número de amostras por atualização de gradiente.\n",
    "   - epochs=50: Número de vezes que o modelo treina em todo o conjunto de dados.\n",
    "   - validation_data=(x_test, y_test): Conjunto de dados para validação durante o treino.\n",
    "   - callbacks=[rlrp]: Lista de callbacks a serem aplicados durante o treino.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.2007 - loss: 2.0084 - val_accuracy: 0.2454 - val_loss: 1.9604 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.2321 - loss: 1.9322 - val_accuracy: 0.2562 - val_loss: 1.8893 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.2761 - loss: 1.8694 - val_accuracy: 0.3117 - val_loss: 1.8034 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.2948 - loss: 1.8150 - val_accuracy: 0.3025 - val_loss: 1.7817 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.3046 - loss: 1.7771 - val_accuracy: 0.3102 - val_loss: 1.7516 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.3182 - loss: 1.7503 - val_accuracy: 0.3210 - val_loss: 1.7280 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.3442 - loss: 1.6977 - val_accuracy: 0.3349 - val_loss: 1.6853 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.3595 - loss: 1.6581 - val_accuracy: 0.3735 - val_loss: 1.6596 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - accuracy: 0.3620 - loss: 1.6212 - val_accuracy: 0.3704 - val_loss: 1.6360 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - accuracy: 0.3885 - loss: 1.5913 - val_accuracy: 0.3735 - val_loss: 1.5905 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.4096 - loss: 1.5554 - val_accuracy: 0.3904 - val_loss: 1.5858 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - accuracy: 0.4011 - loss: 1.5207 - val_accuracy: 0.3951 - val_loss: 1.5512 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.4129 - loss: 1.5096 - val_accuracy: 0.3719 - val_loss: 1.5953 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.4400 - loss: 1.4597 - val_accuracy: 0.4244 - val_loss: 1.5348 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.4445 - loss: 1.4267 - val_accuracy: 0.4136 - val_loss: 1.5250 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - accuracy: 0.4499 - loss: 1.4224 - val_accuracy: 0.4460 - val_loss: 1.4779 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.4854 - loss: 1.3581 - val_accuracy: 0.4506 - val_loss: 1.4319 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.4803 - loss: 1.3529 - val_accuracy: 0.4444 - val_loss: 1.4354 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.4977 - loss: 1.2951 - val_accuracy: 0.4645 - val_loss: 1.4303 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - accuracy: 0.5295 - loss: 1.2594 - val_accuracy: 0.4645 - val_loss: 1.4190 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - accuracy: 0.5068 - loss: 1.2667 - val_accuracy: 0.4614 - val_loss: 1.4288 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.5511 - loss: 1.1997 - val_accuracy: 0.4614 - val_loss: 1.3697 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.5438 - loss: 1.1862 - val_accuracy: 0.4784 - val_loss: 1.3990 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.5651 - loss: 1.1452 - val_accuracy: 0.4830 - val_loss: 1.3477 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.5761 - loss: 1.1414 - val_accuracy: 0.4676 - val_loss: 1.3682 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.5907 - loss: 1.0880 - val_accuracy: 0.4830 - val_loss: 1.3777 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.6007 - loss: 1.0550 - val_accuracy: 0.5015 - val_loss: 1.2998 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.6145 - loss: 1.0511 - val_accuracy: 0.5170 - val_loss: 1.2767 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.6321 - loss: 0.9908 - val_accuracy: 0.4938 - val_loss: 1.3008 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.6387 - loss: 0.9908 - val_accuracy: 0.5247 - val_loss: 1.2878 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.6662 - loss: 0.9070 - val_accuracy: 0.5231 - val_loss: 1.3330 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - accuracy: 0.6736 - loss: 0.8837 - val_accuracy: 0.5139 - val_loss: 1.3152 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.6731 - loss: 0.8711 - val_accuracy: 0.5231 - val_loss: 1.3094 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.6754 - loss: 0.8645 - val_accuracy: 0.5463 - val_loss: 1.3061 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.6897 - loss: 0.8279 - val_accuracy: 0.5478 - val_loss: 1.3097 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.6878 - loss: 0.8260 - val_accuracy: 0.5278 - val_loss: 1.2771 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.6957 - loss: 0.8228 - val_accuracy: 0.5448 - val_loss: 1.3677 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.6948 - loss: 0.8258 - val_accuracy: 0.5756 - val_loss: 1.2918 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7133 - loss: 0.7506 - val_accuracy: 0.5247 - val_loss: 1.4559 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7291 - loss: 0.7380 - val_accuracy: 0.5417 - val_loss: 1.2605 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.7377 - loss: 0.6966 - val_accuracy: 0.5509 - val_loss: 1.4111 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7600 - loss: 0.6432 - val_accuracy: 0.5494 - val_loss: 1.2357 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.7543 - loss: 0.6807 - val_accuracy: 0.5833 - val_loss: 1.2115 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.7652 - loss: 0.6295 - val_accuracy: 0.5571 - val_loss: 1.3612 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.7530 - loss: 0.6463 - val_accuracy: 0.5849 - val_loss: 1.3865 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7731 - loss: 0.6026 - val_accuracy: 0.5864 - val_loss: 1.2819 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7781 - loss: 0.5835 - val_accuracy: 0.5849 - val_loss: 1.3042 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.7797 - loss: 0.5722 - val_accuracy: 0.5679 - val_loss: 1.3920 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.7996 - loss: 0.5541 - val_accuracy: 0.5802 - val_loss: 1.3143 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.7894 - loss: 0.5530 - val_accuracy: 0.5849 - val_loss: 1.3383 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1548f0d07d0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(\n",
    "    X_train, Y_train, batch_size=64, epochs=50, validation_data= (X_test,Y_test),\n",
    "    callbacks=[ReduceLROnPlateau(monitor= 'loss', factor= 0.4, patience= 2, min_lr= 0.0000001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Avaliar e Visualizar a Performance do Modelo\n",
    "\n",
    "##### Passo 1: Avaliando o Modelo\n",
    "- **Precisão nos Dados de Teste**:\n",
    "  - Use `model.evaluate(x_test, y_test)` para calcular a precisão do modelo no conjunto de teste.\n",
    "\n",
    "##### Passo 2: Preparando os Gráficos\n",
    "- **Definindo Épocas**:\n",
    "  - Crie uma lista de épocas para o eixo x\n",
    "- **Configurando o Layout do Gráfico**:\n",
    "  - Use `fig, ax = plt.subplots(1, 2)` para criar dois gráficos lado a lado.\n",
    "\n",
    "##### Passo 3: Plotando a Perda\n",
    "- **Gráfico de Perda**:\n",
    "  - Plote a perda de treino e teste:\n",
    "    ```python\n",
    "    ax[0].plot(epochs, train_loss, label='Training Loss')\n",
    "    ax[0].plot(epochs, test_loss, label='Testing Loss')\n",
    "    ```\n",
    "\n",
    "##### Passo 4: Plotando a Precisão\n",
    "- **Gráfico de Precisão**:\n",
    "  - Plote a precisão de treino e teste:\n",
    "    ```python\n",
    "    ax[1].plot(epochs, train_acc, label='Training Accuracy')\n",
    "    ax[1].plot(epochs, test_acc, label='Testing Accuracy')\n",
    "    ```\n",
    "\n",
    "##### Objetivos:\n",
    "  - **Perda**: Ajuda a identificar se o modelo está treinando bem ou se há overfitting.\n",
    "  - **Precisão**: Mostra o quão eficaz é o treinamento do modelo em acertar as previsões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Chegou a parte boa! vamos prever os valores nos dados de teste\n",
    "- Passo 1: Use model.predict() no x_test e salve o resultado em pred_test.\n",
    "- Passo 2: Crie y_pred a partir de pred_test usando inverse_transform do encoder (Precisamos converter as previsões codificadas do One-Hot de volta aos rótulos originais)\n",
    "- Passo 3: Faça o mesmo para os rótulos de teste (y_test), decodificando-os de volta aos rótulos originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rode essa célula para ver se o modelo que criamos está fazendo sentido para a maioria dos valores.\n",
    "\n",
    "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df['Predicted Labels'] = y_pred.flatten()\n",
    "df['Actual Labels'] = y_test.flatten()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Plotar Matriz de Confusão\n",
    "- Agora tenho um desafio para você, eu quero que você crie uma matriz de confusão que relacione os resultados preditos com os valores reais das emoções!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a função classification_report do sklearn para visualizar a precisão, recall e f1-score do modelo.\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusão\n",
    "- Podemos ver que nosso modelo é mais preciso na predição das emoções surpresa e raiva, o que faz sentido, pois os arquivos de áudio dessas emoções diferem bastante dos outros em aspectos como tom, velocidade, etc.\n",
    "- No geral, alcançamos 62% de precisão nos dados de teste, o que é razoável, mas podemos melhorar ainda mais aplicando mais técnicas de aumento de dados e utilizando outros métodos de extração de features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Salvando o Modelo e o Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passo 1: Salvando o Modelo\n",
    "1. **Importação:** Use o `load_model` do Keras.\n",
    "2. **Diretório:** Crie um diretório chamado `models` se não existir.\n",
    "3. **Salvar:** Salve o modelo como no caminho especificado.\n",
    "\n",
    "##### Passo 2: Salvando o Scaler\n",
    "1. **Importação:** Use `joblib`.\n",
    "2. **Diretório:** Utilize o mesmo caminho `models`.\n",
    "3. **Salvar:** Salve o scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
